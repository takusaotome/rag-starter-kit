# =============================================================================
# Universal RAG API - Environment Configuration Example
# =============================================================================
# Copy this file to .env and customize the values for your specific use case

# =============================================================================
# REQUIRED SETTINGS
# =============================================================================

# OpenAI API Key (Required)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# JWT Secret Key (Required for authentication)
# Generate a secure random string for production use
JWT_SECRET_KEY=58acd9fe76b15c20d2f1c9e764b52dc31c22541f869876b4c5aa46e478c4def8

# =============================================================================
# CORE PATHS CONFIGURATION
# =============================================================================

# Path to your knowledge base directory
# This directory should contain your domain-specific .md files
KNOWLEDGE_PATH=knowledge

# Path to your prompt configuration directory
# Contains the prompt.yaml file that defines your AI assistant's behavior
PROMPTS_PATH=prompt

# Name of the prompt configuration file
# Allows you to use different prompt files for different domains
PROMPT_FILE=prompt.yaml

# Path where the vector store will be saved/loaded
# The processed embeddings of your knowledge base
VECTOR_STORE_PATH=vector_store

# =============================================================================
# RAG PROCESSING SETTINGS
# =============================================================================

# Document Processing Settings
# Size of text chunks for embedding (in tokens)
CHUNK_SIZE=800

# Overlap between consecutive chunks (in tokens)
# Helps maintain context across chunk boundaries
CHUNK_OVERLAP=100

# Number of relevant chunks to retrieve for each query
# Higher values provide more context but may introduce noise
RETRIEVAL_K=6

# =============================================================================
# LLM CONFIGURATION
# =============================================================================

# OpenAI model to use for generating responses
# Latest models (2025): gpt-4o, gpt-4o-mini, gpt-4-turbo, o1-preview, o1-mini
# Recommended: gpt-4o (balanced performance/cost), gpt-4o-mini (cost-effective)
LLM_MODEL=gpt-4o

# Temperature setting for response generation (0.0 to 1.0)
# Lower values (0.1-0.3) = more factual, deterministic responses
# Higher values (0.7-0.9) = more creative, varied responses
LLM_TEMPERATURE=0.3

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# JWT Algorithm for token signing
JWT_ALGORITHM=HS256

# Server host and port
HOST=0.0.0.0
PORT=8000

# =============================================================================
# DOMAIN-SPECIFIC CONFIGURATION EXAMPLES
# =============================================================================

# Example 1: Medical Knowledge Base
# KNOWLEDGE_PATH=knowledge/medical
# PROMPTS_PATH=prompts/medical
# PROMPT_FILE=medical_assistant.yaml
# VECTOR_STORE_PATH=vector_store/medical
# LLM_TEMPERATURE=0.1

# Example 2: Legal Document Assistant
# KNOWLEDGE_PATH=knowledge/legal
# PROMPTS_PATH=prompts/legal
# PROMPT_FILE=legal_assistant.yaml
# VECTOR_STORE_PATH=vector_store/legal
# LLM_TEMPERATURE=0.2

# Example 3: Technical Documentation
# KNOWLEDGE_PATH=knowledge/tech_docs
# PROMPTS_PATH=prompts/technical
# PROMPT_FILE=tech_support.yaml
# VECTOR_STORE_PATH=vector_store/technical
# RETRIEVAL_K=8
# LLM_TEMPERATURE=0.4

# Example 4: Customer Support
# KNOWLEDGE_PATH=knowledge/support
# PROMPTS_PATH=prompts/support
# PROMPT_FILE=customer_service.yaml
# VECTOR_STORE_PATH=vector_store/support
# CHUNK_SIZE=600
# RETRIEVAL_K=4
# LLM_TEMPERATURE=0.5

# Example 5: Educational Content
# KNOWLEDGE_PATH=knowledge/education
# PROMPTS_PATH=prompts/education
# PROMPT_FILE=tutor_assistant.yaml
# VECTOR_STORE_PATH=vector_store/education
# LLM_MODEL=gpt-3.5-turbo
# LLM_TEMPERATURE=0.6

# =============================================================================
# DEVELOPMENT & TESTING
# =============================================================================

# Uncomment for development mode with more verbose logging
# LANGCHAIN_VERBOSE=true
# LOG_LEVEL=DEBUG

# Uncomment to use different models for cost optimization during development
# LLM_MODEL=gpt-3.5-turbo

# =============================================================================
# PRODUCTION CONSIDERATIONS
# =============================================================================

# For production deployment, consider:
# 1. Use a strong, randomly generated JWT_SECRET_KEY
# 2. Store sensitive values in a secure secret management system
# 3. Set appropriate CHUNK_SIZE and RETRIEVAL_K based on your use case
# 4. Monitor API usage and costs with OpenAI
# 5. Implement rate limiting and authentication as needed
# 6. Use environment-specific configuration files