# PMBOK-GPT Knowledge Base Design

# PMBOK-GPT Knowledge Base Design

*Figure 1: **RAG Architecture for PMBOK-GPT.** The PMBOK 6th and 7th edition documents are ingested and split into chunks with metadata, then embedded and stored in a vector index. At query time, the user’s query is embedded and used to retrieve relevant chunks (preferring 7th edition by default via metadata filtering). Retrieved context is fed into a prompt template for the LLM (e.g. GPT-4) to generate an answer, optionally citing sources. An optional reranker or hybrid search can refine the retrieved chunks for relevance.*

## Chunk-Level RAG Design (Indexing & Metadata)

**Chunking Strategy:** All source documents from *A Guide to the PMBOK®* 6th and 7th Editions should be broken into manageable chunks before indexing. Smart chunking improves retrieval accuracy by keeping each piece semantically coherent and not too large for the LLM context window. We can use LangChain’s text splitters (e.g. `RecursiveCharacterTextSplitter`) to split the text by sections or paragraphs while respecting semantic boundaries. A typical fixed chunk size might be on the order of \~1000 characters with some overlap (e.g. 100–200 characters overlap) to maintain continuity between chunks. Smaller chunks enable more fine-grained and precise matching to queries, whereas overly large chunks can introduce irrelevant noise and reduce retrieval accuracy. However, chunks shouldn’t be *too* small to avoid losing context – the ideal size balances relevance and context. For example, one might start with \~500-1000 tokens per chunk (with overlap) and adjust based on testing. Overlap ensures that important concepts cut at a boundary still appear in at least one chunk in full.

**Embedding Method:** Each chunk is converted to a vector embedding to enable semantic search. OpenAI’s state-of-the-art embedding model (e.g. *text-embedding-ada-002*) is a strong choice for quality, but open-source embeddings (like Sentence Transformers models from HuggingFace) can perform comparably with the right model selection. In fact, domain-tuned models (e.g. `sentence-transformers/all-mpnet-base-v2`) combined with a good reranker (e.g. a cross-encoder for relevance) have been shown to deliver *“top-tier performance…against any OpenAI embeddings”* in RAG scenarios. OpenAI embeddings offer ease of use and generally excellent semantic capture out-of-the-box, while HuggingFace models allow offline deployment and fine-tuning if needed. A pragmatic approach is to start with OpenAI for reliability, then evaluate open-source models for cost or data privacy reasons – potentially even fine-tuning a model on project management text if higher accuracy is required.

**Metadata Design:** Each chunk is annotated with rich metadata to facilitate targeted retrieval. At minimum, include the PMBOK *edition* (6 or 7) as metadata on every chunk. Additional metadata keys can encode the structural context of that chunk: for PMBOK 6th Edition, tag the *Knowledge Area* (e.g. Scope, Schedule, Risk) and *Process Group* (Initiating, Planning, Executing, M\&C, Closing) relevant to that text, as well as the specific *process name* if applicable. For PMBOK 7th Edition, use metadata for the *Performance Domain* (e.g. Stakeholders, Team, Delivery, etc.) or principle related to that chunk, since the 7th edition is organized by 12 principles and 8 performance domains rather than knowledge areas. It’s also useful to record section or chapter titles, so the assistant can cite or identify the source section in responses. For example, a chunk from the 6th Edition *Scope Management* chapter might have metadata `{edition: 6, knowledge_area: "Scope", process_group: "Planning", section: "5.4 Create WBS"}`. A chunk from 7th Edition might have `{edition: 7, domain: "Delivery Performance", principle: "Focus on Value"}` if those apply. This metadata allows the system to filter or prioritize results by edition or topic. It also provides interpretable context if the assistant needs to mention where the info came from (e.g. “according to the Stakeholder performance domain in PMBOK 7…”).

*Figure 2: **Chunking and Metadata Example.** A section of PMBOK text is split into semantically coherent chunks. Each chunk carries metadata such as edition, Knowledge Area (KA) and Process Group (PG) for 6th Edition chunks or the relevant Performance Domain for 7th Edition chunks. This enables filtering and targeted retrieval (e.g. only 7th edition chunks, or only chunks from a certain knowledge area).*

**Indexing Strategy:** All chunk embeddings are stored in a vector database (e.g. Chroma, Pinecone, Weaviate) with the metadata attached. A single unified index can be used for all chunks, leveraging metadata filters to distinguish editions. This is often simpler than maintaining separate indexes per edition, because a unified vector store can still return the most relevant chunks across the entire knowledge base while allowing post-filtering. For instance, by default the query may apply a filter `edition:7` to prefer only 7th Edition results. If nothing relevant is found or if the user specifically asks for 6th Edition info, the filter can be adjusted or removed. Metadata filtering is supported by many vector databases to narrow results by structured attributes, which we can use for queries like “find relevant chunks where edition=7 AND knowledge\_area=Risk”. Additionally, to improve search **recall**, we might use hybrid search or multiple indexes: semantic search finds conceptually relevant text, while a keyword or BM25 search on the raw text could catch exact terms (especially if a user uses a specific PMBOK term). Some modern vector stores support hybrid queries mixing vector similarity with keyword conditions. We can also implement **re-ranking** as a second stage: retrieve the top *k* chunks by embedding similarity, then use a cross-encoder model or LLM to re-rank those by how well they actually answer the query. Reranking helps filter out tangential chunks that were retrieved and ensures the final context passages are highly relevant to the question, reducing noise before the LLM sees them. This two-stage retrieval (dense retrieval + LLM rerank) is a proven best practice to boost precision of RAG pipelines. In LangChain, one could integrate this by using the `mmr=True` option for maximal marginal relevance re-ranking or by manually scoring candidate texts with an LLM. The index should also store an identifier or reference for each chunk (like a source citation pointer), so that the final answer can include a citation (e.g. “PMBOK7, p.123”) for verification.

## Representative Prompt Templates

Designing robust **prompt templates** is crucial so that the LLM uses the retrieved PMBOK knowledge effectively for various use cases. We outline a few representative prompt structures:

### **Prompt for Audit Checklist Generation**

For generating an audit or review checklist based on PMBOK guidelines, the prompt should instruct the LLM to produce a structured list of checkpoints derived from the context. For example:

- **System/Instruction:** *“You are an AI Project Management Assistant. Using the PMBOK context provided, generate an **audit checklist** of key items to review for the specified project process or knowledge area. The checklist should cover all essential PMBOK recommendations for compliance.”*

- **Context Insertion:** (Here we would insert relevant chunks from the knowledge base, e.g. the PMBOK processes for that area).

- **User Query Example:** *“Create an audit checklist for Project Risk Management in a project.”*

- **Format Hint:** The prompt can suggest a bullet list format: *“Provide the checklist as a list of bullet points, each describing one control or deliverable to verify.”*

This template ensures the LLM understands it should output a checklist. The context would include PMBOK content on Risk Management processes (like *Plan Risk Management, Identify Risks, Perform Qualitative Risk Analysis*, etc.), so the model can extract items like *“Ensure a risk management plan is created and approved”*, *“Verify a risk register exists and is regularly updated”*, etc. The prompt emphasizes using the **context** so that each item is grounded in PMBOK’s guidance, yielding a high-confidence, verifiable checklist rather than an invented one.

### **Prompt for Executive Report Summary**

When summarizing a project report for executives, the prompt should focus on clarity, brevity, and high-level impacts. For instance:

- **System/Instruction:** *“You are an AI assistant tasked with writing an **executive summary** of a project status report. Using the context below (which contains details from the project report and PMBOK guidelines for effective executive communication), provide a concise summary suitable for senior executives.”*

- **Context:** Here we would supply relevant context, such as the raw text of the project status report and possibly any PMBOK advice on status reporting or communication management (e.g. guidelines from the *Communications Management* knowledge area on tailoring information to stakeholders).

- **User Prompt:** *“Summarize the Q3 Project Alpha status report for the executive steering committee.”*

- **Focus:** The template should instruct the model to highlight key points only: *“Emphasize overall project health, major milestones achieved, current risks/issues, and any decisions needed, in no more than 4-5 sentences.”*

This guides the LLM to produce a high-level summary. By providing PMBOK context on effective communication, we ensure the style aligns with best practices (e.g. being clear and outcome-focused). The summary should avoid technical jargon, focus on value and decisions (as PMBOK 7th edition’s principles suggest focusing on outcomes and value), and remain succinct. If the user only provides the report and not PMBOK context explicitly, the system can still retrieve general guidance on executive communications from the knowledge base to inform the tone and structure.

### **Prompt for Problem-Solving Advice (with Citations)**

For advisory questions (e.g. “How should I handle scope creep in my project?”), the prompt template should encourage a **context-informed solution with cited PMBOK references**:

- **System/Instruction:** *“You are a Project Management expert assistant. The user has a specific problem. Provide a detailed solution or advice, **grounded in PMBOK guidelines**, and include references to the PMBOK where applicable.”*

- **Context:** Relevant chunks from PMBOK are inserted, such as sections on *Change Control* and *Scope Management* if the question is about scope creep.

- **User Question:** e.g. *“Our project is suffering from scope creep. What steps should we take to control it?”*

- **Answer Guidance:** The prompt can add: *“Cite the sources from PMBOK that support your recommendations (for example, ‘PMBOK6 - Scope Management, p. 130’). Explain the reasoning clearly.”*

In the answer, the LLM should then use the provided PMBOK context (like the process of *Perform Integrated Change Control, Scope Change procedures,* etc.) to form an action plan. The answer might say, for example: *“First, implement a formal change control process to evaluate scope changes (ensure a Change Control Board is in place). According to PMBOK, all scope changes should go through integrated change control with stakeholder approval. Next, revisit the scope management plan and communicate the scope baseline to all stakeholders to prevent uncontrolled changes…”* Each step would have justification drawn from the context, with a citation placeholder. The prompt’s emphasis on including the citation text ensures high transparency – the user can see exactly which PMBOK section the advice came from. This pattern of *“context snippet + reasoning + \[citation]”* yields an answer that is both helpful and trusted, suitable for internal PM education where users may want to verify the source in the PMBOK.

### **Prompt Design for Flexibility and Educational Tone**

All prompt templates should be designed to allow the AI some flexibility in phrasing and adapting to user needs, rather than rigidly quoting PMBOK text. The 7th Edition in particular emphasizes principles and outcomes over prescriptive processes, so the assistant’s tone should be explanatory and *practical*. We achieve this by instructing the model to *“explain in simple terms”* or *“provide practical examples”* when appropriate. For example, after providing context, the prompt might say: *“Using the information above, answer the question in a way that a practitioner can easily understand – you may rephrase the formal PMBOK language into more straightforward advice while still conveying the key points. Avoid just copying text; instead, interpret and apply it to the scenario.”* This ensures the output isn’t a dry excerpt from the guide, but rather an application of PMBOK knowledge to the user’s situation, which is more engaging for learning purposes. The prompts can also include an encouragement for the model to be thorough: *“If relevant, include brief examples or clarify why these practices matter.”*

By balancing fidelity to PMBOK with accessible explanations, the AI outputs become both **educational** (teaching the user PM concepts) and **practically useful**. The underlying prompt templates in LangChain can be implemented as `PromptTemplate` objects with slots for the retrieved context and the user’s query. Each template will have slight variations tailored to the task (checklist vs. summary vs. advice), but all will share the common instruction to ground answers in the provided PMBOK content.

## Conflict Resolution Logic for PMBOK Versions

Handling differences between the 6th and 7th Editions is critical so that the assistant provides the **most up-to-date guidance** while acknowledging legacy terms when needed. The system will follow a version prioritization rule: **by default, prefer PMBOK 7th Edition content as the primary source**, since it reflects the latest standards and principles. PMBOK 7’s perspective is broader and principle-based (outcome-focused), whereas PMBOK 6 was very process-oriented. Thus, for a general query (not specifying an edition), the retrieval will filter or rank results to favor edition=7 chunks first. This ensures the answer aligns with the current guidance (for example, emphasizing value delivery, adaptability, and principles).

However, the 7th Edition dramatically restructured the body of knowledge – the ten Knowledge Areas from the 6th Edition were replaced by eight Performance Domains in the 7th. Many detailed tools/processes from the 6th edition are still valid but are not explicitly detailed in the 7th’s principle-focused text. Therefore, the assistant will **not discard PMBOK 6 content entirely**. If a user’s question uses terminology from the 6th Edition (e.g. *“critical path method”*, *“Manage Quality process”*, or \*“What are the 5 process groups?”), the system will recognize this and include the relevant 6th Ed chunks to address it. In implementation, this could mean if an initial search (edition=7) yields too few results or low confidence, the system performs a second search allowing edition=6, or simply searches all and then filters/uses the best matches overall. The knowledge base remains inclusive of both editions because PMI has indicated that **6th Edition information is still considered part of the foundational knowledge** (it even remains testable for the PMP exam). The assistant should thus respond accurately even for legacy concepts, while gently guiding the user toward the modern viewpoint if appropriate.

To resolve conflicting guidance between editions, the assistant will follow these rules:

- **Default to 7th Edition Practices:** If both editions cover the topic, the answer is framed using the 7th Edition’s concepts as the primary viewpoint (since it’s more up-to-date). For example, a question on “project success criteria” might be answered in terms of value and outcomes (7th Ed emphasis) rather than just the iron triangle metrics from earlier editions. The content from the 6th Ed can still be used as supplementary, but the core answer leans on the latest edition’s philosophy.

- **Acknowledge Differences When Asked or When Relevant:** If the user specifically asks, *“What’s the difference between PMBOK 6 and 7 on X?”* or if the context suggests possible confusion between editions, the assistant will explicitly mention the distinction. For instance: *“**Version Note:** In PMBOK 6th Ed., risk management was a detailed process-driven Knowledge Area with processes like Plan Risk Management, Identify Risks, etc., whereas PMBOK 7th Ed. doesn’t list these processes but instead embeds risk concepts under broader principles and the ‘Uncertainty’ Performance Domain. So, while the practices (identifying and analyzing risks) remain important, the 7th Edition approaches them at a higher level of principles.”* This kind of statement helps the user see how the guidance evolved. We design the prompt (or a separate chain) to include edition indicators in the context or to have the LLM label which edition a snippet comes from, enabling it to articulate such comparisons.

- **Preventing Contradiction:** In cases where 6th and 7th Edition might use different terms or emphasize different approaches, the retrieval might fetch from both. The prompt can instruct the LLM to reconcile them. For example, if the question is about “quality management,” PMBOK 6 has a process *Manage Quality* and specific tools, whereas PMBOK 7 discusses quality under the Performance Domain of *Delivery* and principles of *Quality*. The assistant would combine these by first giving the up-to-date principle (“ensure quality through continuous improvement and stakeholder satisfaction as per 7th Ed”) and then, if needed, noting the concrete processes from 6th Ed (“formerly, PMBOK 6 detailed this via processes like Manage Quality and Control Quality which involve quality audits, control charts, etc., which are techniques still applicable” – possibly provided if the user is looking for actionable techniques).

- **Edition Citation:** Each chunk knows its source, so the answer can cite accordingly (e.g. labeling a reference “\[PMBOK6]” vs “\[PMBOK7]”). If the user requests to see differences, the answer itself can include phrases like “(PMBOK 6th Edition)” and “(PMBOK 7th Edition)” to make it clear which source is being quoted or referenced. This explicit labeling within the answer can be triggered by a flag in the prompt. For instance, an advanced option is to have a boolean parameter `show_version_diff` which, when true, modifies the prompt to say “Include a brief note comparing any differing guidance between editions.”

In the LangChain pipeline, this conflict resolution logic can be implemented by custom logic in the retriever or the QA chain. One approach is a **custom Retriever** that queries the vector store with a primary filter for edition 7, but falls back to edition 6 if needed (or queries both and merges results). Another approach is to always retrieve a mix but then in a **post-processing step**, drop older chunks unless the query or context suggests they’re needed. We can also store a mapping of key 6th edition terms to their 7th edition equivalents (for example, map each Knowledge Area to the relevant Performance Domain) to help the system fetch the right context. For example, if a question mentions *“Schedule Management”* (a 6th Ed knowledge area), the system could automatically also look at *“Delivering on Schedule”* or related 7th Ed domain content. This can be done via a synonym table or by issuing two queries behind the scenes.

Finally, we ensure that **if the user specifically asks for a certain edition** (e.g. “According to PMBOK 6, what is X?”), the system respects that by filtering the retrieval to the requested edition exclusively. The prompt would also mirror the phrasing (“According to PMBOK 6, ...”) so that the LLM’s answer focuses only on that edition’s content in the response.

In summary, the PMBOK-GPT knowledge base is designed to be edition-aware: prioritizing the most current guidance (7th Ed) for forward-looking advice, but gracefully handling legacy terminology and pointing out differences when relevant. This ensures users get accurate, up-to-date answers without losing the rich detail from the prior edition. All these design choices – from chunking with edition metadata, to prompt instructions about version nuances – contribute to a reliable AI assistant for project management training and support, grounded firmly in the PMBOK knowledge base.

**Sources:** The above design is informed by best practices in Retrieval-Augmented Generation for robust knowledge bases, including guidance on chunk sizing and overlap, embedding model comparisons, the use of metadata and hybrid search for improved retrieval, and official differences between PMBOK editions as documented by PMI. This approach ensures high fidelity to source material and adaptability to user needs, fulfilling the goal of a PMBOK-driven AI assistant that is both **authoritative** and **practically helpful**.
